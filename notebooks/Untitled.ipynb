{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "DATA_INTERIM_PATH = DATA_PATH + 'interim/'\n",
    "train = pd.read_csv(DATA_INTERIM_PATH + 'train_p.csv')\n",
    "val = pd.read_csv(DATA_INTERIM_PATH + 'val_p.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmill/anaconda3/envs/SemEval2019-4/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "from models.feature_spaces import create_tfidf, create_avg_word_embeddings\n",
    "%aimport models.feature_spaces\n",
    "\n",
    "from datatasks.sample_data import sample_data\n",
    "\n",
    "from datatasks.new_preprocess import tokenize\n",
    "%aimport datatasks.new_preprocess\n",
    "\n",
    "from models.pipeline import make_features_pipeline\n",
    "%aimport models.pipeline\n",
    "\n",
    "from models.models import run_models\n",
    "%aimport models.models\n",
    "\n",
    "from models.plot import plot_LSA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s = sample_data(train, 50000, 'train')\n",
    "val_s = sample_data(val, 10000, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(997, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s = tokenize(train_s)\n",
    "val_s = tokenize(val_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = \"../data/external/GoogleNews-vectors-negative300.bin.gz\"\n",
    "transformer = create_avg_word_embeddings(word2vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = create_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = make_features_pipeline(transformer, 'tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_s.drop('hyperpartisan', axis=1)\n",
    "y_train = train_s['hyperpartisan']\n",
    "X_test = val_s.drop('hyperpartisan', axis=1)\n",
    "y_test = val_s['hyperpartisan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49997, 16) (49997,)\n",
      "Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.72      0.43      0.54      4999\n",
      "       True       0.59      0.84      0.70      5000\n",
      "\n",
      "avg / total       0.66      0.63      0.62      9999\n",
      "\n",
      "Accuracy: 0.6330\n",
      "Best model is Logistic Regression with an accuracy score of 0.6330\n"
     ]
    }
   ],
   "source": [
    "model_list = ['lr']\n",
    "best_tfidf_model, best_tfidf_model_type, best_tfidf_model_predictions = run_models(feats, model_list, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "all_words = [word for tokens in train_s[\"tokens\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in train_s[\"tokens\"]]\n",
    "VOCAB = sorted(list(set(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "vectors = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 35\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT=.2\n",
    "\n",
    "def create_cnn_data(df, vectors):\n",
    "    \n",
    "    all_words = [word for tokens in train_s[\"tokens\"] for word in tokens]\n",
    "    VOCAB = sorted(list(set(all_words)))\n",
    "    VOCAB_SIZE = len(VOCAB)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "    tokenizer.fit_on_texts(df[\"preprocessed_text\"].tolist())\n",
    "    sequences = tokenizer.texts_to_sequences(df[\"preprocessed_text\"].tolist())\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    cnn_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    labels = to_categorical(np.asarray(df[\"hyperpartisan\"]))\n",
    "\n",
    "    indices = np.arange(cnn_data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    cnn_data = cnn_data[indices]\n",
    "    labels = labels[indices]\n",
    "    num_validation_samples = int(VALIDATION_SPLIT * cnn_data.shape[0])\n",
    "\n",
    "    embedding_weights = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "    for word,index in word_index.items():\n",
    "        embedding_weights[index,:] = vectors[word] if word in vectors else np.random.rand(EMBEDDING_DIM)\n",
    "    print(embedding_weights.shape)\n",
    "    \n",
    "    return cnn_data, labels, embedding_weights, word_index,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 275014 unique tokens.\n",
      "(275015, 300)\n"
     ]
    }
   ],
   "source": [
    "cnn_data, labels, embedding_weights, word_index = create_cnn_data(train_s, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = cnn_data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = cnn_data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "model = ConvNet(embedding_weights, MAX_SEQUENCE_LENGTH, len(word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(train[\"hyperpartisan\"].unique())), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39998 samples, validate on 9999 samples\n",
      "Epoch 1/3\n",
      "39998/39998 [==============================] - 31s 785us/step - loss: 0.5492 - acc: 0.6941 - val_loss: 0.4741 - val_acc: 0.7584\n",
      "Epoch 2/3\n",
      "39998/39998 [==============================] - 32s 797us/step - loss: 0.4392 - acc: 0.7792 - val_loss: 0.4465 - val_acc: 0.7732\n",
      "Epoch 3/3\n",
      "39998/39998 [==============================] - 32s 799us/step - loss: 0.3828 - acc: 0.8183 - val_loss: 0.4509 - val_acc: 0.7755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c25ca5ba8>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=3, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
